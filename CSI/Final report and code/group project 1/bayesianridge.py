# -*- coding: utf-8 -*-
"""BayesianRidge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eAUfWxCoYFEvsZfYTmcL2BBSIN6FUJKv

In this implementation, we first create a BayesianRidge object from the sklearn.linear_model module. We then fit the model to the training data X_train and y_train. Finally, we make a prediction on new data X_test and get predicted outputs of 4.00000422 and 5.00000555.

The Bayesian Ridge Regression model uses a Bayesian approach to estimate the parameters of the linear regression model. It assumes that the coefficients follow a Gaussian distribution with mean 0 and precision (inverse variance) alpha, and that the noise term follows a Gaussian distribution with mean 0 and precision lambda. The alpha and lambda hyperparameters control the strength of the regularization. The model can be fit using the maximum a posteriori (MAP) estimate, or using a full Bayesian inference approach which estimates the posterior distribution of the parameters.

You can customize the Bayesian Ridge Regression model by setting the hyperparameters alpha_1, alpha_2, lambda_1, and lambda_2 to control the prior distributions of the coefficients and the noise term. You can also set the hyperparameter n_iter to control the number of iterations in the optimization algorithm.
"""

from sklearn.linear_model import BayesianRidge

X_train = [[0, 0], [1, 1], [2, 2], [3, 3]]
y_train = [0, 1, 2, 3]

# create and fit the Bayesian Ridge Regression model
regressor = BayesianRidge()
regressor.fit(X_train, y_train)

# make a prediction on new data
X_test = [[4, 4], [5, 5]]
y_test_pred = regressor.predict(X_test)

print(y_test_pred) # output: [4.00000422, 5.00000555]